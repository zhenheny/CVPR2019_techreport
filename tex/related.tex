
\vspace{-0.5\baselineskip}
\section{Related Work}
\vspace{-0.5\baselineskip}
\label{sec:related}
%\vspace{-0\baselineskip}
In this section, we briefly overview some traditional methods, and introduce current SOTA methods for unsupervised single view 3D geometry recovery and edge detection. 

\textbf{~Structure from motion and single view geometry.}
Geometric based methods estimate 3D from a given video with feature matching, such as SFM~\cite{wu2011visualsfm}, SLAM~\cite{mur2015orb,engel2014lsd} and DTAM~\cite{NewcombeLD11}, which could be effective and efficient in many cases. 
However, they can fail at where there is low texture, or drastic change of visual perspective \etc. More importantly, it can not extend to single view reconstruction.
Specific rules are developed for single view geometry, such as computing vanishing point~\cite{HoiemEH07}, following rules of BRDF~\cite{prados2006shape,kong2015intrinsic}, or extract the scene layout with major plane and box representations~\cite{DBLP:conf/iccv/SchwingFPU13,DBLP:conf/3dim/SrajerSPP14} \etc. These methods can only obtain sparse geometry representations, and some of them require certain assumptions (\textit{e.g.} Lambertian, Manhattan world).

\textbf{Supervised single view geometry via CNN.}
Deep neural networks (DCN) developed in recent years, \eg VGG~\cite{simonyan2014very} and ResNet~\cite{laina2016deeper}, provide strong feature representation. Dense geometry, i.e., pixel-wise depth and normal maps, can be readily estimated from a single image~\cite{wang2015designing,eigen2015predicting,laina2016deeper,li2017two,chuang2018cvpr}. The learned CNN model shows significant improvement compared to other methods based on hand-crafted features~\cite{karsch2014depth,ladicky2014pulling,zeisl2014discriminatively}. Others tried to improve the estimation further by appending a conditional random field (CRF)~\cite{DBLP:conf/cvpr/WangSLCPY15,Liu_2015_CVPR,li2015depth}. Recently, Wang \etal \cite{peng2016depth} proposed a depth-normal regularization over large planar surfaces, which is formulated based on a dense CRF~\cite{DBLP:journals/corr/abs-1210-5644}, yielding better results on both depth and normal predictions. However, all these methods require densely labeled ground truths, which are expensive to obtain in natural environments.

 % Long-range context and semantic cues are also incorporated in later works to refine the dense prediction by combining the networks with conditional random fields (CRF)~\cite{DBLP:conf/cvpr/WangSLCPY15, Liu_2015_CVPR, li2015depth, Wang_2015_CVPR}. Most recently, Eigen \textit{et al.}~\cite{DBLP:conf/iccv/EigenF15} further integrate depth and normal estimation into a large multi-scale network structure, which significantly improves the geometry estimation accuracy. Nevertheless, the output of the networks still lacks regularization over planar surfaces due to the adoption of pixel-wise loss functions during network training, resulting in unsatisfactory experience in 3D image editing applications.


%Later, the depth estimation from single-view input image is inherently an ill-posed problem which can only be solved with priors and semantic understanding of the scene - tasks that the convnets are good at. 
% There is a chain of works that propose learning depth/normal maps with deep convnets supervised with dense ground truth maps. \cite{liu2016learning} proposed to combine a convnet with a superpixel-based conditional random field (CRF) model, and to learn unary and pairwise terms for the depth map. Ladicky \etal \cite{ladicky2014pulling} incorporate semantics into their model to improve the per pixel depth estimation. Karsch \etal \cite{karsch2014depth} attempt to produce more consistent image level predictions by copying whole image depth from the training set, which requires the entire training set to be available during test time. Eigen \etal \cite{eigen2014depth,eigen2015predicting} showed it is possible to produce dense per pixel depth estimation through a two-scale deep network, trained on images and their corresponding ground truth depth maps. Many works have built upon this method using techniques like CRFs to improve accuracy \cite{li2015depth}, replacing regression loss with classification loss \cite{cao2017estimating}, implementing more robust loss function \cite{laina2016deeper}.

% Wang \etal \cite{wang2015designing} also built upon the basic idea of \cite{eigen2014depth} and incorporate scene geometrical priors to help learning, in the related problem of normal estimation. Data-driven normal estimation has not been long since the first approach, that directly tries to estimate surface normals from the data was proposed by Fouhey \etal \cite{fouhey2013data}.  Both this work and \cite{zeisl2014discriminatively} proposed by Ladicky \etal used hand-crafted features like texton, SIFT, local quantized ternary patterns. Li \etal \cite{\cite{li2015depth}} first proposed to estimate depth and normal jointly and showed performance gain. 

\textbf{Unsupervised single view geometry.}
Motivated by traditional methods, videos, which are easier to obtain and hold richer 3D information. Motivated by traditional methods like SFM and DTAM, lots of CNN based methods are proposed to do single view geometry estimation with supervision from vieos, and yield impressive progress. 
 Deep3D \cite{xie2016deep3d} learns to generate the right view from the given left view by supervision of stereo image pairs. In order to do back-propagation on depth values, the depth space is quantized and it is trained to select the right depth value. 
% Video holds a great potential towards semantically meaningful visual representations. Recently, there have been a small number of deep network based methods for depth estimation. 
% DeepStereo \cite{flynn2016deepstereo} introduced a novel view synthesis network, while . This network generates new view images by selecting pixels from nearby images. The most appropriate depth values are selected to sample pixel values from neighboring images, based on plane sweep volume.
Concurrently, Garg \etal \cite{GargBR16} applied the similar supervision from stereo pairs, while the depth is kept continuous. They apply Taylor expansion to approximate the gradient for depth. Godard \etal \cite{godard2016unsupervised} extend Garg's work by including depth smoothness loss and left-right depth consistency. 
Zhou \etal \cite{zhou2017unsupervised} incoporated camera pose estimation into the training pipeline, which made depth learning possible from monocular videos. And they came up with an explainability mask to relieve the problem of moving object in rigid scenes.
%However, they only focus on rigid scene without the ability to deal with moving objects. 
At the same time, Vijayanarasimhan \etal \cite{Vijayanarasimhan17} proposed a network to include the modeling of rigid object motion. Most recently, Yang \etal \cite{yang2018aaai} further induce normal representation, and proposed a dense depth-normal consistency within the pipeline, which not only better regularizes the predicted depths, but also learns to produce a normal estimation. However, as discussed in \secref{sec:intro}, the regularization is only applied locally and can be blocked by image gradient, yielding false geometrical discontinuities inside a smooth surface.

% bilateral filter, dense crf
\textbf{Non-local smoothness.} Long range and non-local spatial regularization has been vastly explored in classical graphical models like CRF~\cite{lafferty2001conditional}, where nodes beyond the neighboring are connected, and the smoothness in-between are learned with high-order CRF~\cite{ye2009conditional} or densely-connected CRF~\cite{DBLP:conf/icml/KraehenbuehlK13}. They show superior performance in detail recovery than those with local connections in multiple tasks, \eg segmentation~\cite{DBLP:journals/corr/abs-1210-5644}, image disparity~\cite{scharstein2007learning} and image matting~\cite{chen2013image} \etc In addition, efficient solvers are also developed such as fast bilateral filter~\cite{barron2016fast} or permutohedral lattice~\cite{adams2010fast}. 

Although these methods run effectively and could combine with CNN as a post processing component~\cite{arnab2016higher,crfasrnn_iccv2015,peng2016depth,wang2017occlusion}, they are not very efficient in learning and inference when combined with CNN, due to the iterative loop. To some extent, the non-local information from CRF overlaps with those multi-scale strategies~\cite{zhao2016pyramid,ChenPSA17} proposed recently, which yield comparable performance while are more effective. Thus, we adopt the latter strategy to learn the non-local smoothness inside the unsupervised pipeline, which is represented by geometrical edge in our case.


\textbf{Edge detection.}
Learning edges from an image beyond low level methods such as Sobel or Canny~\cite{canny1986computational} has long been explored via supervised learning~\cite{yamaguchi2012continuous,konishi2003statistical,arbelaez2011contour,DollarICCV13edges} along with the growth of semantic edge datasets~\cite{MartinFTM01,hou2013boundary}. Recently, methods~\cite{bertasius2015high,xie2015holistically,DBLP:journals/corr/Kokkinos15} have achieved outstanding performance by adopting supervisedly trained deep features. 

As discussed, high-level edges can also be learned through non-local smoothness by implicit supervision. One recent work close to ours is \cite{chen2016semantic}. They append a spatial domain transfer (DT) component after a CNN, which acts similar to a CRF for smoothness, and improves the results of semantic segmentation. However, their work is fully supervised with ground truth, and similar to CRF, the DT propagates to neighboring pixels every iteration which is not efficient. When no supervision is provided, Li \etal \cite{li2016unsupervised} proposed to use optical flow~\cite{revaud2015epicflow} to explicitly capture motion edge and use it as supervision for edge models. 

Our method discovers geometrical edges in an unsupervised manner. In addition, we show that it is possible for the network to directly extract edge and smoothen the 3D geometry by enforcing a unified regularization, without appending extra components like~\cite{chen2016semantic}. We also show better performance than~\cite{li2016unsupervised} in street-view cases. 
% Lastly, joint with 3D geometry, our edges also contain the information of occlusion which is also full of interest in computer vision~\cite{HoiemEH11,DBLP:conf/cvpr/SundbergBMAM11,wang2016doc}.
% also learning with video

%The edge detection task has a long history. Early edge detection methods were manually designed to use simple convolutional filters such as Sobel \cite{} or Canny \cite{}. More recent works have proposed edge detectiors trained with a data driven manner. Dollar \etal \cite{DollarICCV13edges} proposed a realtime edge detection method using trained structured random forests (SE). With the advances of deep neural network, the performance of dense edge prediction is boosted. A notable work by Xie \etal \cite{xie2015holistically} proposed a holistically-nested edge detector (HED) which trains and predicts the edge in an image-to-image fashion and performs end-to-end-training.  
% To avoid tedious edge ground truth labeling, Li \etal \cite{Li2016unsupervised} proposed to learn motion edge from videos and use the optical flow edges as intermediate ground truth to supervise the training of SE.

% Although there have been many efforts devoted to unsupervised learning of 3D geometry, the geometry consistency, like depth and normal consistency, geometric edge and depth/normal consistency have not been explored in the pipeline. Our work fills in this part and show that edges serve as an important intermediate regularization for 3D geometry learning. 
% Although vastly developed for depth estimation from video, normal information, which is also highly interesting for geometry prediction, has not been considered inside the pipeline. This paper fills in the missing part, and show that normal can serve as a natural regularization for depth estimation, which significantly improves the state-of-the-art performance. Finally, with our designed loss, we are able to learn the indoor geometry where \cite{zhou2017unsupervised} usually fails to estimate.

% To deal with the problem of occlusion boundary caused by the rigid scene transformation and moving object, they proposed to mask out all these boundaries.
