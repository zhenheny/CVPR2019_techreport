% \vspace{-0.2\baselineskip}
\section{Evaluation}
% \vspace{-0.3\baselineskip}
\label{sec:evaluation}
In this section, the datasets and evaluation metrics are firstly introduced. We then describe the implementation details of the framework. Compreshensive experiments have been conducted on to validate the effectiveness of our pipeline on different tasks.

\subsection{Datasets and metrics}
We conducted experiments on object detection and classification. The performances are evaluated on the Charades dataset \cite{sigurdsson2016hollywood}.

\textbf{Datasets.} The activity driven weakly supervised detection task takes video and corresponding action label as training input, and is evaluated on static images with bounding box annotations. Charades dataset is a good choice to both train and evaluate on. It includes 9,848 videos of 157 action classes, among which, 66 are interactive actions with objects. There are on average 6.8 action labels for one video, resulting in a total of 67,000 clips. 7,986 videos (54,000 clips) in training set are used as training samples. The official Charades dataset doesn't provide object bounding box annotations. Instead, we use the spatial bounding box annotations released by \cite{yuan2017temporal}. In the released annotations, 17 objects in the 66 interactive actions are labeled with bounding boxes in the test set videos. The 1,812 test videos are downsampled to 1 frame per second (fps) and then annotated on each frames. There are 3.4 bounding box annotations per frame on average. Although all downsampled frames in test set have been annotated, 5,000 frames from 200 test videos are actually used for evaluation. We follow the same practice (evaluate on 5,000 frames) to directly compare with the current SOTA methods.

\textbf{Evaluation metrics.} We have conducted experiments to evaluate the object detection and image classification performances on the test set. For detection, we report per-class \textit{average precision} (AP) at \textit{intersection-over-union} (IoU) of 0.5 between detection and ground truth boxes, and also mean AP (mAP) as a combined metric. For classification, we report the mAP on frame-level image classification. 

\subsection{Implementation details}
\subsection{Ablation study}
\subsection{Comparison with SOTA}
\subsection{Generalization study}
\begin{table*}[]
\centering
\fontsize{6}{8}\selectfont
\caption{AP performance (\%) on each object class and mAP (\%) comparison with different weakly supervised methods.}
\label{tbl:class_wise}
\begin{tabular}{l|cccccccccccccccccc}
\specialrule{.2em}{.1em}{.1em}
Methods                                        & bed & broom & chair & cup & dish & door & laptop & mirror & pillow & refri & shelf & sofa    & table   & tv   & towel       & vacuum    & window     & mAP(\%)      \\ \hline
WSDDN \cite{bilen2016weakly}                   & 2.38 & 0.04 &1.17 &0.03 & 0.13 & 0.31 & 2.81 & 0.28 & 0.02 & 0.12 & 0.03 & 0.41 & 1.74 & 1.18 & 0.07 & 0.08 & 0.22 & 0.65   \\
ContextLocNet \cite{kantorov2016contextlocnet} & 7.40 & 0.03 & 0.55 & 0.02 & 0.01 & 0.17 & 1.11 &0.66 & 0 & 0.07 & 1.75 & 4.12 & 0.63 & 0.99 & 0.03 & 0.75 & 0.78 & 1.12  \\
TD-LSTM \cite{yuan2017temporal}                & \textbf{9.19} & 0.04 & 4.18 & \textbf{0.49} & 0.11 & 1.17 & 2.91 & 0.30 & 0.08 & 0.29 & 3.21 & 5.86 & 3.35 & 1.27 & 0.09 & 0.60 & 0.47 & 1.98 \\ \hline
Ours                                           & 8.64 & \textbf{3.79} & 7.31 & 0.28 & 1.23 & 5.64 & 5.49 & 1.44 & 2.34 & 8.94 & 4.14 & 8.61 & 6.25 & 1.97 & 0.17 & 3.24 & 0.33 & 4.10\\ \hline
\end{tabular}
\end{table*}



Our method is compared with several baseline methods on the same dataset for both the detection (det) and classification (cls) tasks. The baselines include \cite{bilen2016weakly,kantorov2016contextlocnet} which are weakly supervised object detectors learning from static images, \cite{yuan2017temporal} which learns object detector from videos but only leverages the temporal consistency within the same action class, R*CNN \cite{gkioxari2015contextual} with both pretrained model and model trained on Charades data and Faster R-CNN (pretrained and trained on Charades), which is a fully supervised baseline. The performances are presented in Tab. \ref{tbl:sota}. We also compare current SOTA methods on each object class, which is shown in Tab. \ref{tbl:class_wise}.

\begin{table}[]
\fontsize{7}{8}\selectfont
% \def\arraystretch{1.5}
\setlength{\tabcolsep}{3pt}
\centering
\caption{Performances of different baseline methods on object classification and object detection tasks}
\label{tbl:sota}
\begin{tabular}{l|c|cc}
\specialrule{.2em}{.1em}{.1em}
Methods                          & Supervision                          & mAP (det) & mAP (cls) \\ \hline
WSDNN \cite{bilen2016weakly}                            & \multirow{5}{*}{Weak (action class)} & 0.65            & 15.67                \\
ContextLocNet \cite{kantorov2016contextlocnet}                    &                                      & 1.12            & 16.47                \\
TD-LSTM \cite{yuan2017temporal}                          &                                      & 1.98            & 19.52                \\
R*CNN \cite{gkioxari2015contextual} (pre-trained)            &                                      & $0.47\pm 0.02$\footnotemark[1]            &                      \\
R*CNN \cite{gkioxari2015contextual} (re-trained)                 &                                      & $1.26\pm 0.11$             &                      \\ \hline
Faster R-CNN \cite{ren2015faster} (pre-trained)             & \multirow{2}{*}{Strong (bbox)}       & $4.39\pm 0.34$\footnotemark[1]            & -                    \\
Faster R-CNN \cite{ren2015faster} (fine-tuned) &                                      & $63.98\pm 1.13$                & -                    \\ \hline
\end{tabular}
\end{table}
\addtocounter{footnote}{1}
\footnotetext{The test set is slightly different but numbers should be similar.}



\newpage