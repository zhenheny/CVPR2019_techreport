% \vspace{-0.2\baselineskip}
\section{Evaluation}
% \vspace{-0.3\baselineskip}
\label{sec:evaluation}

We conducted experiments on Charades dataset \cite{sigurdsson2016hollywood}. Charades dataset includes 9848 videos of 157 action classes, among which, 124 actions include interaction with objects. In our experiments, the pipeline is trained on 7342 training videos with action class labes and evaluated on 5,000 test frames with object bounding box annotations \cite{}. The 5,000 frames are randomly picked from 200 test videos. 

Our method is compared with several baseline methods on the same dataset for both the detection (det) and classification (cls) tasks. The baselines include \cite{bilen2016weakly,kantorov2016contextlocnet} which are weakly supervised object detectors learning from static images, \cite{yuan2017temporal} which learns object detector from videos but only leverages the temporal consistency within the same action class, R*CNN \cite{gkioxari2015contextual} with both pretrained model and model trained on Charades data and Faster R-CNN (pretrained and trained on Charades), which is a fully supervised baseline. The performances are presented in Tab. \ref{tbl:sota}. We also compare current SOTA methods on each object class, which is shown in Tab. \ref{tbl:class_wise}.

\begin{table}[]
\fontsize{7}{8}\selectfont
% \def\arraystretch{1.5}
\setlength{\tabcolsep}{3pt}
\centering
\caption{Performances of different baseline methods on object classification and object detection tasks}
\label{tbl:sota}
\begin{tabular}{l|c|cc}
\specialrule{.2em}{.1em}{.1em}
Methods                          & Supervision                          & mAP (det) & mAP (cls) \\ \hline
WSDNN \cite{bilen2016weakly}                            & \multirow{5}{*}{Weak (action class)} & 0.65            & 15.67                \\
ContextLocNet \cite{kantorov2016contextlocnet}                    &                                      & 1.12            & 16.47                \\
TD-LSTM \cite{yuan2017temporal}                          &                                      & 1.98            & 19.52                \\
R*CNN \cite{gkioxari2015contextual} (pre-trained)            &                                      & $0.47\pm 0.02$\footnotemark[1]            &                      \\
R*CNN \cite{gkioxari2015contextual} (re-trained)                 &                                      & $1.26\pm 0.11$             &                      \\ \hline
Faster R-CNN \cite{ren2015faster} (pre-trained)             & \multirow{2}{*}{Strong (bbox)}       & $4.39\pm 0.34$\footnotemark[1]            & -                    \\
Faster R-CNN \cite{ren2015faster} (fine-tuned) &                                      & $63.98\pm 1.13$                & -                    \\ \hline
\end{tabular}
\end{table}
\addtocounter{footnote}{1}
\footnotetext{The test set is slightly different but there should not be large offset in numbers.}

\begin{table}[]
\centering
\caption{Class-wise mAP performance for different methods.}
\label{tbl:class_wise}
\begin{tabular}{l|ccc}
\specialrule{.2em}{.1em}{.1em}
Methods                                        & cup/bottle    & table         & door          \\ \hline
WSDDN \cite{bilen2016weakly}                   & 0.03          & 1.74          & 0.31          \\
ContextLocNet \cite{kantorov2016contextlocnet} & 0.02          & 0.63          & 0.17          \\
TD-LSTM \cite{yuan2017temporal}                & \textbf{0.49} & 3.35          & 1.17          \\ \hline
Ours                                           & 0.28          & \textbf{4.39} & \textbf{2.35} \\ \hline
\end{tabular}
\end{table}

\newpage