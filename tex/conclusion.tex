\section{Conclusion}
Weakly supervised object detection is attracting significant attention as it gets rid of supervision from bounding box annotations, which are expensive and tedious. Exsiting works either focus on learning object appearance representation from static images, which fail to generalize to objects in videos because of domain shift, or only leverage the object class annotations but ignore the action information. In this work, we investigate the problem of weakly supervised object detection with the supervision from video-level action class annotations. We propose to leverage both the object and action class labels during training. Specifically, we leverage the object appearance consistency by applying object classification across different videos or actions which involve the same object class. The temporal consistency is modeled with attention maps which are dependent only on the action class. We also model the action apperance cues by supervision from action class labels. Combining the regularizations above, our method has shown better performance than state-of-the-art (SOTA) methods. We conducted experiments on Charades dataset to compare with baseline methods using the metric of mean Average Precision (mAP) and showed our pipeline outperforms the current SOTA by \%. (To evaluate the generalization capability of our pipeline, we also directly evaluate the model, which is trained on Charades, on different datasets.)