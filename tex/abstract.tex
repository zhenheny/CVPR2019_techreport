
\begin{abstract}
    Weakly supervised object detection is attracting significant attention as it does not require tedious bounding box annotation. There have been exsiting works which focus on learning object appearance representation from static images. However, they often fail to generalize to objects in videos because of domain shift. In this work, we investigate the problem of weakly supervised object detection with the supervision from video-level interactive activity class annotations. 

    Different from learning solely from static images, appereance consistency over time can be leveraged in videos. More importantly, the object apperance representation is further regularized by jointly modeling the appereance information with different activity classes which involve the same object category. 
    We have conducted comprehensive experiments on Charades and (potentially MPII-Cooking) dataset and achieve () performance. To validate the performance of generalization, (the proposed method) is directly tested on (Pascal VOC) dataset and shows superior performance over other baselines.
\end{abstract}
