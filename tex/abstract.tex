
\begin{abstract}
    Weakly supervised object detection is attracting significant attention as it gets rid of supervision from bounding box annotations, which are expensive and tedious. Exsiting works either focus on learning object appearance representation from static images, which fail to generalize to objects in videos because of domain shift, or only leverage the object class annotations but ignore the action information. In this work, we investigate the problem of weakly supervised object detection with the supervision from video-level action class annotations. We propose to leverage both the object and action class labels during training. Specifically, we leverage the object appearance consistency by applying object classification across different videos or actions which involve the same object class. The temporal consistency is modeled with attention maps which are dependent only on the action class. We also model the action apperance cues by supervision from action class labels. Combining the regularizations above, our method has shown better performance than state-of-the-art (SOTA) methods. We conducted experiments on Charades dataset to compare with baseline methods using the metric of mean Average Precision (mAP) and showed our pipeline outperforms the current SOTA by \%. (To evaluate the generalization capability of our pipeline, we also directly evaluate the model, which is trained on Charades, on different datasets.)

    % Different from learning solely from static images, appereance consistency over time can be leveraged in videos. More importantly, the object apperance representation is further regularized by jointly modeling the appereance information with different activity classes which involve the same object category. 
    % We have conducted comprehensive experiments on Charades and (potentially MPII-Cooking) dataset and achieve () performance. To validate the performance of generalization, (the proposed method) is directly tested on (Pascal VOC) dataset and shows superior performance over other baselines.
\end{abstract}
